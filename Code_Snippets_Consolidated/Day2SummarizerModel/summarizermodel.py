# -*- coding: utf-8 -*-
"""SummarizerModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KumUezibHLJXCfkc6FO05XY4d3AIA0Mu
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers
!pip install bert_score

"""#Importing Libraries, Data and Summarizer

"""

import pandas as pd
import numpy as np
from bert_score import score
import json
from transformers import BartTokenizer, BartForConditionalGeneration, pipeline

"""#Importing the Data

"""

# Open the JSON file
with open('/content/drive/MyDrive/scraped_articles.json', 'r') as f:
    # Load the JSON data
    data = json.load(f)

# Convert the JSON data to a DataFrame
df = pd.json_normalize(data)
df.head

"""#DATA CLEANING """

# Identify the column(s) that contain the article content
content_columns = ['title', 'content']

# Identify duplicate rows based on the article content
duplicates = df.duplicated(subset=content_columns, keep='first')

# Remove the duplicate rows from the DataFrame
df = df[~duplicates]

# Print the number of rows remaining in the DataFrame
print(len(df))
df.head

# Remove articles with less than or equal to 30 words
df = df[df['content'].str.split().str.len() > 30]
# Print the number of rows remaining in the DataFrame
print(len(df))
# reset the index of the dataset to start at 0
df = df.reset_index(drop=True)

df.head()

"""#DOWNLOAD BART"""

# Initialize the tokenizer and model
tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn").to("cuda")  # Move the model to GPU

# Create the summarization pipeline with the GPU-enabled model
pipe = pipeline("summarization", model=model, tokenizer=tokenizer, device=0)  # Set device to 0 for GPU

"""#Testing the model on the first article

"""

test = df['content'][0]
pipe_out = pipe(test, max_length=120, min_length=100, do_sample=False)

pipe_out

summarized_text = pipe_out[0]['summary_text']

#check the word length of summarized text
len(summarized_text.split())

"""# SUMMARIZING ARTICLES"""

max_seq_length = 1024  # set the maximum sequence length for the model
segment_length = 500  # set the desired length for each text segment

# loop over the articles and summarize each one
summarized_articles = []
summary_cache = {}  # Initialize the caching mechanism

for article in df['content']:
    # Check if the article is already summarized in the cache
    if article in summary_cache:
        summarized_article = summary_cache[article]
    else:
        # split the article into smaller segments
        segments = [article[i:i+segment_length] for i in range(0, len(article), segment_length)]
        
        # summarize each segment separately and concatenate the summaries
        segment_summaries = []
        for segment in segments:
            if len(segment.split()) < 100:
                pipe_out = pipe(segment, max_length=40, min_length=20, do_sample=False)
                segment_summaries.append(pipe_out[0]['summary_text'])
            
            else:
                pipe_out = pipe(segment, max_length=120, min_length=100, do_sample=False)
                segment_summaries.append(pipe_out[0]['summary_text'])
    
        summarized_article = ' '.join(segment_summaries)
        summary_cache[article] = summarized_article  # Store the summary in the cache

    summarized_articles.append(summarized_article)

df['summarized_articles'] = summarized_articles
df.tail()

"""Clean it again"""

# Define a function to clean text
def clean_text(text):
    # Remove \n
    text = re.sub('\n', ' ', text)
    # Remove \u2019 and \u2026
    text = re.sub('\u2019|\u2026', '', text)
    # Remove any other unwanted characters or symbols
    text = re.sub('[^a-zA-Z0-9\s]', '', text)
    # Remove extra whitespace
    text = re.sub('\s+', ' ', text).strip()
    return text

# Clean the text data in the JSON file
data['content'] = clean_text(data['content'])
data['summarized_articles'] = clean_text(data['summarized_articles'])
data['title'] = clean_text(data['title'])

df['summarized_length'] = df['summarized_articles'].apply(lambda x: len(x.split()))
df['article_length'] = df['content'].apply(lambda x: len(x.split()))
df.tail()

# save the dataframe to a JSON file
df.to_json('/content/drive/MyDrive/summarized_article_interntest.json')

"""CALCULATING THE BERT SCORE"""

#calcualte bert score
P, R, F1 = score(list(df['content']), list(df['summarized_articles']), lang='en', verbose=True)

df['bert_f1_score'] = F1
df['bert_P_score'] = P
df['bert_R_score'] = R

df.head()

